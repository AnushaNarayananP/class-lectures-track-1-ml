{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f1cc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Scrapy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (4.8.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (21.0.0)\n",
      "Requirement already satisfied: tldextract in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (3.2.0)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (18.1.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (1.1.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (3.4.8)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (0.1.16)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (1.21.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (1.0.4)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (2.0.5)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (22.2.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (0.3.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (5.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (61.2.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Scrapy) (1.6.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from cryptography>=2.0->Scrapy) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.0->Scrapy) (2.21)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->Scrapy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from parsel>=1.5.0->Scrapy) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (0.4.8)\n",
      "Requirement already satisfied: attrs>=16.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (21.4.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (15.1.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (21.3.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (20.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (4.1.1)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (1.0.2)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->Scrapy) (3.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tldextract->Scrapy) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tldextract->Scrapy) (2.27.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tldextract->Scrapy) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (1.26.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Scrapy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53822ae2",
   "metadata": {},
   "source": [
    "conda install -c conda-forge scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0dfb698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "775f985c",
   "metadata": {},
   "source": [
    "scrapy startproject tutorial\n",
    "Looks like you are trying the command scrapy startproject stack inside python interactive shell.\n",
    "Run the same command directly on bash shell, and not inside python shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "facad2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\PG Diploma in Data Science\\ML algorithm\\class-lectures-track-1-ml\\Natural Language Processing\\tutorial\n"
     ]
    }
   ],
   "source": [
    "cd tutorial"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3b1efa3",
   "metadata": {},
   "source": [
    "A Spider\n",
    "We will now create a new spider inside tutorial/spiders. A spider is nothing but a python class which inherits from the scrapy.Spider class. Each spider has a unique name which is used when running the spider and a start_urls which a list of URLs the spider starts crawling as soon as it is run; we will set start_urls to [\"http://books.toscrape.com/\"] which is the website we want to scrape. Create a new file at tutorial/spiders/books_spider.py which will have our spider, with the following content."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d236e20b",
   "metadata": {},
   "source": [
    "When a spider is run, it starts by executing the parse function of it on all the start_urls. The parse function has a parameter named response which stores all information about the fetched page. response.url contains URL of the currently fetched page. Lets print the URL in the fetch function and see what it does. (The following code should be inside the BookSpider defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "960b0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class BookSpider(scrapy.Spider):\n",
    "    name = \"books\"\n",
    "\n",
    "    start_urls = [\"http://books.toscrape.com/\"]\n",
    "    def parse(self, response):\n",
    "        print(response.url)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e16a54dd",
   "metadata": {},
   "source": [
    "try recreate your project like this: scrapy startproject junoproject scrapy genspider juno juno.co.uk edit your spider as you posted, and run again. I can run this absolutely fine â€“ \n",
    "Anzel\n",
    " Oct 14, 2014 at 11:49 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce65bcb9",
   "metadata": {},
   "source": [
    "scrapy crawl books"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4649fa1",
   "metadata": {},
   "source": [
    "Let's now run the spider. Go to base project directory (tutorial) and run (Note that books is the name we gave the spider inside the BookSpider class)\n",
    "\n",
    "scrapy crawl books"
   ]
  },
  {
   "cell_type": "raw",
   "id": "754cd830",
   "metadata": {},
   "source": [
    "CSS and XPath selectors\n",
    "Now, lets focus on actually extracting the content out of the website. If you have ever used CSS, you will know about selectors which are used to apply style to specific elements. We can use the same type of selectors to extract data from specific elements. This can be done by using response.css(\".css selector here\") and then use getall() or get() function of the CSS object to actually get the element. You can use ::text in the selector to get the text contained in the required element. To get a specific property of the element, say href, use attrib[\"href\"] on the object returned by response.css.\n",
    "\n",
    "A more powerful type of selectors are XPath selectors. Here is a cheat-sheet for XPath and here is a more complete tutorial. A summary of XPath selectors that will be used in this tutorial are:\n",
    "\n",
    "//element: Get all elements of type element anywhere in the DOM.\n",
    "element[@class='classname']]: Get only elements of type element and of class classname. Note the class is matched entirely, if a element is of class class1 class2, a @class='class1' will not match it.\n",
    "element[contains(@class, 'classname')]: Get elements of type element whose class contains classname anywhere.\n",
    "element/text(): Get the text contained inside elements.\n",
    "element/@href: Get the href attribute (can be replace with any attribute)\n",
    "element1/element2: Get element2 which a direct child of element1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fecb5517",
   "metadata": {},
   "source": [
    "cat_names stores the list of all categories and cat_urls the corresponding URLs. We can iterate over these using zip. There is one issue though, the URLs are relative to the current URL; to fix this, we use response.urljoin to get the absolute URL. Now that we have the URLs, we need a way to scrape them separately, the parse function is specifically made to get the list of categories; hence, we need a separate function which will parse all books in a category - we call this function parse_category. To tell scrapy to parse a particular URL, we need to create an object of scrapy.Request and return it. Since we have a list of URLs to return, we'll use yield instead of return to return multiple Requests to scrapy. A scrapy.Request object required two parameters - the URL and the function to pass the handle to after a response is received, called callback. We use another parameter cb_kwargs to pass additional parameters to the callback function instead of just the response. Here is the parse function after adding all the above mentioned features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
