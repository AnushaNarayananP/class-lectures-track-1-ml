{
 "cells": [
  {
   "cell_type": "raw",
   "id": "12bac0a7",
   "metadata": {},
   "source": [
    "scrapy startproject tutorial"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0b904d5",
   "metadata": {},
   "source": [
    "bash shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49311a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Practical_Natural_Language_Processing\\tutorial\n"
     ]
    }
   ],
   "source": [
    "cd tutorial"
   ]
  },
  {
   "cell_type": "raw",
   "id": "533a2bcb",
   "metadata": {},
   "source": [
    "A Spider\n",
    "We will now create a new spider inside tutorial/spiders. A spider is nothing but a python class which inherits from the scrapy.Spider class. Each spider has a unique name which is used when running the spider and a start_urls which a list of URLs the spider starts crawling as soon as it is run; we will set start_urls to [\"http://books.toscrape.com/\"] which is the website we want to scrape. Create a new file at tutorial/spiders/books_spider.py which will have our spider, with the following content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173bb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class BookSpider(scrapy.Spider):\n",
    "    name = \"books\"\n",
    "\n",
    "    start_urls = [\"http://books.toscrape.com/\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de901714",
   "metadata": {},
   "source": [
    "When a spider is run, it starts by executing the parse function of it on all the start_urls. The parse function has a parameter named response which stores all information about the fetched page. response.url contains URL of the currently fetched page. Lets print the URL in the fetch function and see what it does. (The following code should be inside the BookSpider defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f12595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class BookSpider(scrapy.Spider):\n",
    "    name = \"books\"\n",
    "\n",
    "    start_urls = [\"http://books.toscrape.com/\"]\n",
    "    def parse(self, response):\n",
    "        print(response.url)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed18d717",
   "metadata": {},
   "source": [
    "scrapy genspider books books.toscrape.com/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c66cc596",
   "metadata": {},
   "source": [
    "scrapy crawl books"
   ]
  },
  {
   "cell_type": "raw",
   "id": "462fc77e",
   "metadata": {},
   "source": [
    "Let's now run the spider. Go to base project directory (tutorial) and run (Note that books is the name we gave the spider inside the BookSpider class)\n",
    "\n",
    "scrapy crawl books"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab70eafb",
   "metadata": {},
   "source": [
    "CSS and XPath selectors\n",
    "Now, lets focus on actually extracting the content out of the website. If you have ever used CSS, you will know about selectors which are used to apply style to specific elements. We can use the same type of selectors to extract data from specific elements. This can be done by using response.css(\".css selector here\") and then use getall() or get() function of the CSS object to actually get the element. You can use ::text in the selector to get the text contained in the required element. To get a specific property of the element, say href, use attrib[\"href\"] on the object returned by response.css.\n",
    "\n",
    "A more powerful type of selectors are XPath selectors. Here is a cheat-sheet for XPath and here is a more complete tutorial. A summary of XPath selectors that will be used in this tutorial are:\n",
    "\n",
    "//element: Get all elements of type element anywhere in the DOM.\n",
    "element[@class='classname']]: Get only elements of type element and of class classname. Note the class is matched entirely, if a element is of class class1 class2, a @class='class1' will not match it.\n",
    "element[contains(@class, 'classname')]: Get elements of type element whose class contains classname anywhere.\n",
    "element/text(): Get the text contained inside elements.\n",
    "element/@href: Get the href attribute (can be replace with any attribute)\n",
    "element1/element2: Get element2 which a direct child of element1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37f8c6ef",
   "metadata": {},
   "source": [
    "Extracting Data\n",
    "Let's take a look at the website we want to scrape. It has a sidebar which shows the list of all categories and on the right we can see all books.\n",
    "\n",
    "The approach used in this tutorial is going to each of the categories and scraping all books inside them.\n",
    "\n",
    "A great tool to test and experiment with selectors in real time is the Scrapy shell. Launch the scrapy shell for website of interest using the following command.\n",
    "\n",
    "scrapy shell \"http://books.toscrape.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5878bfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IPython in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (8.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (0.1.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (3.0.20)\n",
      "Requirement already satisfied: backcall in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (61.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (0.4.4)\n",
      "Requirement already satisfied: stack-data in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from IPython) (2.11.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jedi>=0.16->IPython) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stack-data->IPython) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stack-data->IPython) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from stack-data->IPython) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from asttokens->stack-data->IPython) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install IPython"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8441c2f7",
   "metadata": {},
   "source": [
    "From the shell, you can view the exact response it received using view(response) (make sure you do this for any website you want to scrape before proceeding, the website may look different to your browser and scrapy since scrapy does not have JavaScript by default).\n",
    "\n",
    "Now, let's see how to select the list of categories. We will use the ever useful Inspect Element tool. It can be seen in the source that the category list is in a div of class side_categories. This div contains an unordered list whose only element contains another unordered list! The second ul is the one we want since each element of this contains an anchor tag with the URL to each category. Thus, the XPaths are as follows (you can experiment with XPaths in the scrapy shell till you get the output you need).\n",
    "\n",
    "        cat_names = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/text()\").getall()\n",
    "        cat_urls = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/@href\").getall()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15569207",
   "metadata": {},
   "source": [
    "cat_names = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/text()\").getall()\n",
    "cat_urls = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/@href\").getall()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7437390",
   "metadata": {},
   "source": [
    "cat_names stores the list of all categories and cat_urls the corresponding URLs. We can iterate over these using zip. There is one issue though, the URLs are relative to the current URL; to fix this, we use response.urljoin to get the absolute URL. Now that we have the URLs, we need a way to scrape them separately, the parse function is specifically made to get the list of categories; hence, we need a separate function which will parse all books in a category - we call this function parse_category. To tell scrapy to parse a particular URL, we need to create an object of scrapy.Request and return it. Since we have a list of URLs to return, we'll use yield instead of return to return multiple Requests to scrapy. A scrapy.Request object required two parameters - the URL and the function to pass the handle to after a response is received, called callback. We use another parameter cb_kwargs to pass additional parameters to the callback function instead of just the response. Here is the parse function after adding all the above mentioned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6425f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response= \"file:///C:/Users/LENOVO/AppData/Local/Temp/tmpvptt9lp5.html\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "732fe4a7",
   "metadata": {},
   "source": [
    "Since there are quite a few categories, we limit the number of categories crawled.â€¦\n",
    "\n",
    "Now, let's scrape all the books in a category which will be done inside the parse_category function. You can use scrapy shell again on one of the category URLs to build selectors interactively. Here, we can see that each book is inside an article of class product_pod, which has a h3 containing an anchor tag linking to the book's URL. Thus, the line to get all books will be as follows.\n",
    "\n",
    "        book_urls = response.xpath(\"//article[@class='product_pod']/h3/a/@href\").getall()\n",
    "Now we can loop through the book URLs and yield a scrapy.Request for each URL with the callback as a new function parse_book which we will define later. You would have noticed that some categories have too many books to be displayed in one page and as such they are paginated i.e., separated into pages and there's a next button near the bottom of the page. After yielding all the requests for books we find the next button and get its URL which is then used to yield another request but with the callback being parse_category (which is nothing but a recursion). The entire code for this function is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23d65c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookSpider(scrapy.Spider):\n",
    "    name = \"books\"\n",
    "\n",
    "    start_urls = [\"http://books.toscrape.com/\"]\n",
    "    response= \"file:///C:/Users/LENOVO/AppData/Local/Temp/tmpvptt9lp5.html\"\n",
    "    def parse(self, response):\n",
    "        num_cats_to_parse = 5\n",
    "        cat_names = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/text()\").getall()\n",
    "        cat_urls = response.xpath(\"//div[@class='side_categories']/ul/li/ul/li/a/@href\").getall()\n",
    "        for _, name, url in zip(range(num_cats_to_parse), cat_names, cat_urls):\n",
    "            name = name.strip()\n",
    "            url = response.urljoin(url)\n",
    "            yield scrapy.Request(url,\n",
    "                                 callback=self.parse_category,\n",
    "                                 cb_kwargs=dict(cat_name=name))\n",
    "    def parse_category(self, response, name):\n",
    "        book_urls = response.xpath(\"//article[@class='product_pod']/h3/a/@href\").getall()\n",
    "\n",
    "        for book_url in book_urls:\n",
    "            book_url = response.urljoin(book_url)\n",
    "            yield scrapy.Request(book_url, callback=self.parse_book,\n",
    "                                 cb_kwargs=dict(cat_name=name))\n",
    "\n",
    "        next_button = response.css(\".next a\")\n",
    "        if next_button:\n",
    "            next_url = next_button.attrib[\"href\"]\n",
    "            next_url = response.urljoin(next_url)\n",
    "            yield scrapy.Request(next_url,\n",
    "                                 callback=self.parse_category,\n",
    "                                 cb_kwargs=dict(cat_name=name))\n",
    "    def parse_book(self,response,name):\n",
    "        title_names=response.xpath(\"//instock[@class='title']/ul/li/ul/li/a/text()\").getall()\n",
    "        title_urls=response.xpath(\"//instock[@class='title']/ul/li/ul/li/a/@href\").getall()\n",
    "        price_names=response.xpath(\"//instock[@class='price']/ul/li/ul/li/a/text()\").getall()\n",
    "        price_urls=response.xpath(\"//instock[@class='price']/ul/li/ul/li/a/@href\").getall()\n",
    "        instock_names=response.xpath(\"//instock[@class='stock_information']/ul/li/ul/li/a/text()\").getall()\n",
    "        instock_urls=response.xpath(\"//instock[@class='stock_information']/ul/li/ul/li/a/@href\").getall()\n",
    "        star_names=response.xpath(\"//instock[contains(@class, 'star_rating')]/h2/a/text()\").getall()\n",
    "        star_urls=response.xpath(\"//instock[contains(@class, 'star_rating')]/h2/a/@href\").getall()\n",
    "        for  t_name, t_url,p_name,p_url,i_name,i_url,s_name,s_url in zip(title_names, title_urls, price_names,price_urls,instock_names,instock_urls,star_names,star_urls):\n",
    "            t_url = response.urljoin(t_url)\n",
    "            p_url = response.urljoin(p_url)\n",
    "            i_name = i_name.strip()\n",
    "            i_url = response.urljoin(i_url)\n",
    "            s_url = response.urljoin(s_url)\n",
    "            yield scrapy.Request([t_url,p_url,i_url,s_url],\n",
    "                                 cb_kwargs=dict(cat_name=name))\n",
    "\n",
    "        next_button = response.css(\".next a\")\n",
    "        if next_button:\n",
    "            next_url = next_button.attrib[\"href\"]\n",
    "            next_url = response.urljoin(next_url)\n",
    "            yield scrapy.Request(next_url,\n",
    "                                 callback=self.parse_book,\n",
    "                                 cb_kwargs=dict(cat_name=name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea0eca4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BookSpider.parse of <BookSpider 'books' at 0x1b8493140a0>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=BookSpider()\n",
    "c.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac1797ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BookSpider.parse_category of <BookSpider 'books' at 0x1b8493140a0>>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.parse_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5acc7065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BookSpider.parse_book of <BookSpider 'books' at 0x1b8493140a0>>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.parse_book"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bff0f1b",
   "metadata": {},
   "source": [
    "Finally, let's write the parse_book function to scrape each book. For the purposes of this tutorial, we will only scrape the title, price, stock information and star rating. You can use the scrapy shell to build selectors for them. Getting title and price is trivial. The instock selector gives a list of strings which mostly consist of spaces and newline with the actual information contained in the middle, thus we use strip and join to get the required information. Obtaining the star rating is tricky since that information is only contained in the class name, thus we use XPath to get the class name and then select the second word in that. Finally, we return the required information in a dictionary, the reason for this is that scrapy considers these dictionaries as items. This allows you to directly export the data to a JSON using -o output.json options while running the spider. The main reason to return items is to use scrapy pipelines to programmatically process the data. For example, you could write a pipeline to automatically insert the data into a database or write it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41651e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class BookCsvPipeline():\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open(\"output.csv\", \"at\")\n",
    "        fieldnames = [\"title\",\n",
    "                      \"price\",\n",
    "                      \"stock\",\n",
    "                      \"rating\",\n",
    "                      \"category\"]\n",
    "        self.writer = csv.DictWriter(self.file, fieldnames=fieldnames)\n",
    "        if self.file.tell() == 0:\n",
    "            self.writer.writeheader()\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.writer.writerow(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87fc87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=BookCsvPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6aa84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.open_spider(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e65ef12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.close_spider(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7789f8ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_book\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36mBookCsvPipeline.process_item\u001b[1;34m(self, item, spider)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, item, spider):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\csv.py:154\u001b[0m, in \u001b[0;36mDictWriter.writerow\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwriterow(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\csv.py:147\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dict_to_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextrasaction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 147\u001b[0m         wrong_fields \u001b[38;5;241m=\u001b[39m \u001b[43mrowdict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m                              \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "d.process_item(c.parse_book,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c5df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
